export const projects = [
  {
    title: "LMAOCaT: Low-rank Mamba and gated Attention Optimization",
    date: "Oct 2024 - Dec 2024",
    description: [
      "Recreated LoLCATs and developed a hybrid attention framework integrating Gated Linear Attention and Mamba blocks into Llama 3.2 1B, achieving O(n) inference scaling while maintaining 34.5% accuracy on HellaSwag",
      "Implemented attention transfer and Low-Rank Adaptation (LoRA) for efficient fine-tuning, demonstrating superior performance of hybrid configurations over linear-only substitutions"
    ],
    github: "https://github.com/shitanshubhushan/Linearizing-Llama-3.2-1B/tree/main",
    tags: ["Machine Learning", "NLP", "PyTorch"],
    featured: true
  },
  // ... rest of your projects
]; 